The dataset used is available on github (in the notebook you can find the link) and it consists of reviews of the Disneyland parks of three different regions of the world The work done can be divided into two main steps: preprocessing of the reviews using NLP techniques and 'nltk' library, and training and test of the dataset on the basis of a deep learning model. The model used consists of convolutional 1D layers followed by a GRU layer, this model is taken from the paper "fake news detection based on deep learning" (pdf uploaded). The dataset contained ther important and relevant features such as the city in which the clients visited the park, the time in which they went there and their nationality but they weren't used, for a better and more precise result such features should be added. Moreover, using a simple GRU instead of combining it with convolutional layers might be a better solution.
